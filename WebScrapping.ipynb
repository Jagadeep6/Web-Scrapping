{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca79b969",
   "metadata": {},
   "source": [
    "Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82c099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8067018",
   "metadata": {},
   "source": [
    "Note: User-Agent has been removed from the code. (since its not a generic user agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a50932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The url's of the product pages for scrapping into the list for ease usage\n",
    "url = [ 'https://www.flipkart.com/search?q=furniture&sid=wwe%2Cfc3&as=on&as-show=on&otracker=AS_QueryStore_OrganicAutoSuggest_1_3_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_3_na_na_na&as-pos=1&as-type=RECENT&suggestionId=furniture%7CWardrobes+%26+More&requestId=6468df7f-c9bc-46af-84a1-c58b35c81552&as-backfill=on', 'https://www.flipkart.com/search?q=Computer%20components&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off']\n",
    "HEADERS = ({'User-Agent':, 'Accept-Language':'en-US, en;q=0.5'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9c102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below helps us retrive data from a Product page, kindly uncomment to review the product page\n",
    "\n",
    "def return_data(link):\n",
    "    link = link.get('href')\n",
    "    \n",
    "    product_link = \"https://www.flipkart.com\"+ link\n",
    "    #print(product_link)\n",
    "    new_webpage = requests.get(product_link, headers=HEADERS)\n",
    "    if(new_webpage == 503): #error handling\n",
    "        print('Response 503')\n",
    "        return ['Error']\n",
    "    new_soup = BeautifulSoup(new_webpage.content, 'html.parser') #using beautifulsoup to praise html text\n",
    "    p_name = new_soup.find(\"span\", attrs={'class': 'B_NuCI'}).text.strip() #product name\n",
    "    i_link = new_soup.find(\"img\", attrs={'class': '_396cs4 _2amPTt _3qGmMb'})['src'] #image link\n",
    "    price = new_soup.find('div', attrs={'class': '_30jeq3 _16Jk6d'}).text #price\n",
    "    price_cleaned = re.sub(r'[^\\d.]', '', price)\n",
    "    price = float(price_cleaned) #removing unnecessary char's from price\n",
    "    rating = new_soup.find('div', attrs={'class': '_3LWZlK'}).text #rating of the product\n",
    "    des = new_soup.find('div', attrs={'class': '_1mXcCf RmoJUa'}).text #description of product\n",
    "    review_title = new_soup.find('p', attrs={'class':'_2-N8zT'}).text #review title\n",
    "    review = new_soup.find('div', attrs = {'class': 't-ZTKy'}).text #review\n",
    "    read_more_index = review.find(\"READ MORE\") # if \"READ MORE\" is found, removing it along with any trailing space\n",
    "    if read_more_index != -1:\n",
    "        review = review[:read_more_index].strip()\n",
    "\n",
    "    info = [p_name, i_link, price, rating, des, review_title, review]\n",
    "    \n",
    "    return info #returns a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5339328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data from all the pages \n",
    "#we can extract data from multiple pages without manually inputing links of next pages\n",
    "def data_all_pages(pages, no_of_products):\n",
    "    if(len(pages) == 0):\n",
    "        print(\"No links exist in Pages\")\n",
    "        return []\n",
    "    category_data = []\n",
    "    for i in pages:\n",
    "        sub_page = \"https://www.flipkart.com\"+ i\n",
    "        #print(sub_page)\n",
    "        sub_response = requests.get(sub_page, headers=HEADERS)\n",
    "        if(sub_response.status_code == 503):\n",
    "            print(\"503 Error encountered. Breaking the loop.\")\n",
    "        sub_soup = BeautifulSoup(sub_response.content, 'html.parser')\n",
    "        product_links = sub_soup.find_all(\"a\", attrs={'class' : 's1Q9rs'})\n",
    "        for link in product_links:\n",
    "            try:\n",
    "                category_data.append(return_data(link))\n",
    "            except AttributeError as e: #if some of the data is empty we will get an error here\n",
    "                #error handling\n",
    "                #print(f\"In {link} Attribute error occurred: {e}\")\n",
    "                continue\n",
    "            if(len(category_data) == no_of_products): \n",
    "                return category_data\n",
    "    return category_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d124d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs: list of URL's, file_names to save the file, Number of products to extract\n",
    "#outputs: Generates .csv files\n",
    "\n",
    "def get_data(urls, file_names, no_of_products):\n",
    "    if(len(urls) != len(file_names)):\n",
    "        print(\"Size of urls and file_names doesnt match\")\n",
    "        return\n",
    "    i_fn = 0 #to iterate over the file names\n",
    "    for url in urls: \n",
    "        #print(url)\n",
    "        response = requests.get(url, headers=HEADERS) #request to the server\n",
    "        if(response.status_code == 503): #error handling\n",
    "            print(\"503 Error encountered. Breaking the loop.\")\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        div = soup.find('div', class_='_2MImiq')\n",
    "        a_tags = div.find_all('a')\n",
    "        a_tags.pop()\n",
    "        all_pages = [tag['href'] for tag in a_tags if 'href' in tag.attrs]\n",
    "        data = data_all_pages(all_pages, no_of_products)\n",
    "        with open(file_names[i_fn], 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            print(f\"Writing file to {file_names[i_fn]}\") #for checking progress in case of multiple link\n",
    "            i_fn+=1\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(['Product Name', 'Image Link', 'Price', 'Rating', 'Description', 'Review Title', 'Review'])\n",
    "            for item in data:\n",
    "                csvwriter.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8caf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f29c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file to furniture.csv\n",
      "Writing file to computer_componenets.csv\n"
     ]
    }
   ],
   "source": [
    "get_data(url, ['furniture.csv', 'computer_componenets.csv'], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56053ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
